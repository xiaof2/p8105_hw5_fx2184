---
title: "p8105_hw5_fx2184"
author: "Fei"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(janitor)
library(readr)
library(rvest)
library(purrr)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# Problem 1 

First import the data in individual spreadsheets contained in `./data/zip_data/`. A dataframe is created that includes the list of all files in that directory and the complete path to each file. Next, `map` over paths and import data using the `read_csv` function. Finally, `unnest` the result of `map`.

```{r, message=FALSE, warning=FALSE}
full_df = 
  tibble(
    files = list.files("./data/"),
    path = str_c("./data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```

```{r, warning=FALSE, message=FALSE}
# Tidy the data
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

```{r, warning=FALSE, message=FALSE}
# Make a spaghetti plot showing observations on each subject over time
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group) + 
  labs(x = "Week", y = "Outcome", 
       title = "Observations on Each Subject over 8 Weeks in Two Groups")
```


# Problem 2
```{r, meassage = FALSE}
# read the data from the web 
url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"

homicides_raw = read_csv(url)
```
* The raw dataset include `r nrow(homicides_raw)`observations of homicides reported criminal homicides over the past decade in 50 of the largest American cities, and `r ncol(homicides_raw)` key variables, they are `r homicides_raw %>% names`. 
* some reported `unknown` under variable victim age, sex and race. 
* the `lat` contains `r sum(is.na(homicides_raw$lat))` missing value, and `lon` contains `r sum(is.na(homicides_raw$lon))` missing value, 
* the raw data contains 3 numeric variables: reported data. longitude and latitude, rest of the variables are designed as character variables. 
* The data contains only one observation from `Tulsa` with state `OK`, which does not make sense. So I assume it's a typo, I will delete this row in the further cleaning step. 

## Clean the raw data and create city_state and resolved
```{r}
homicides = homicides_raw %>% 
  janitor::clean_names() %>% 
  mutate(city_state = str_c(city, state,sep = ", "),
         resolved = case_when(
           disposition == "Closed by arrest" ~ "solved",
           disposition ==  "Open/No arrest" ~ "unsolved",
           disposition ==  "Closed without arrest" ~ "unsolved"
         )) %>% 
  filter(city_state != "Tulsa, AL") %>% 
  relocate(uid, city_state)
```

```{r}
#summarize within cities to obtain the total number of homicides and the number of unsolved homicides
total_homicides = homicides %>% 
  group_by(city_state) %>% 
  summarize(total_homicide = n(), unsolved = sum(resolved == "unsolved")) 

total_homicides %>% knitr::kable(align='c',col.names = c("City/State","Total Number of Homicids","Number of Unsolved "))
```


```{r}
# For the city of Baltimore, MD,estimate the proportion of homicides that are unsolved
prop_baltimore =  prop.test(total_homicides %>% filter(city_state == "Baltimore, MD") %>% pull(unsolved),
                            total_homicides %>% filter(city_state == "Baltimore, MD") %>% pull(total_homicide))
broom::tidy(prop_baltimore)[,c(1,5,6)] 

```

The estimate proportion of unsolved homicides in Baltimore, MD is `r prop_baltimore$estimate` with confidence interval 
```{r}
# Estimate proportion of homicides that are unsolved for each of the cities 
# extract both the proportion of unsolved homicides and the confidence interval for each.
prop_cities = 
  total_homicides %>% 
  mutate(prop_test = purrr::map2(.x = unsolved, .y = total_homicide, ~prop.test(x = .x, n = .y)),
         tidy_test = purrr::map(.x = prop_test, ~broom::tidy(.x))) %>% 
  unnest(tidy_test) %>% 
  select(city_state, estimate, conf.low, conf.high)

prop_cities%>% knitr::kable(align='c',col.names = c("City/State","Estimate","Confidence(lower)","Confidence(upper)"))
```

## Create a plot showing the estimates and CI

```{r}
prop_cities%>%
  mutate(city_state = fct_reorder(city_state, estimate)) %>%
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  labs(x = "City",
       y = "Estimate",
       title = "Estimates and Confidence Intervals of Porportion of Unsolved Homicides for Each City") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```
# Problem 3: Explore power in a one-sample t-test.

```{r,warning=FALSE,message=FALSE}
set.seed(1)
```
## Generate the function 
```{r}
ttest = function(n = 30, mu, sigma = 5) {

  sim_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma),
  )
  
  test_data =  t.test(sim_data, mu = 0, conf.level = 0.95) 
  
  sim_data %>% 
    summarize(
      mu_hat = pull(broom::tidy(test_data), estimate),
      p_value = pull(broom::tidy(test_data), p.value)
    )
}
```

## Generate 5000 datasets from the model $X \sim Normal[\mu, \sigma]$ 

```{r}
set.seed(1)

sim_result = tibble(mu = c(0:6)) %>% 
              mutate(
                output_list = map(.x = mu, ~rerun(5000,ttest(mu=.x))),
                estimate_df = map(output_list, bind_rows)
              ) %>% 
    select(-output_list) %>% 
    unnest(estimate_df)
```

##  Association between effect size and power.

```{r}
#showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ
#on the x axis. Describe the association between effect size and power.
sim_result %>% 
  group_by(mu) %>% 
  summarize(rej_num = sum(p_value <0.05),
            total = n(),
            rej_prop = rej_num/total) %>% 
  ggplot(aes(x=mu, y = rej_prop))+
  geom_point(alpha = 0.5)+
  geom_line()+
  geom_text(aes(label = round(rej_prop,3)), vjust = -1, size = 3)+
  labs(
    title = "Association Between Effect Size and Power",
    x = "True Mean", y = "Power of Test"
  )+
  scale_x_continuous(n.breaks = 6)+
  scale_y_continuous(n.breaks = 10)

```

* We could see a positive associate between the effect size and power of the test. 
* With the increase of the different between true mean and effect size, the proportion of times the null was rejected shows an overall increase trend, and getting slow down when effect size becoming larger and the proportion getting close to 1. 

```{r}
# Make a plot showing the average estimate of μ̂， on the y axis and the true value of μ，  on the x axis. 
sim_result %>% 
  group_by(mu)%>% 
  summarize(average_mu = mean(mu_hat)) %>% 
  ggplot(aes(x = mu, y=average_mu))+
  geom_point()+
  geom_line()+
  geom_text(aes(label = round(average_mu,3)),vjust = -1, size = 2)+
  scale_x_continuous(breaks = 1:6)+ 
  scale_y_continuous(breaks = 1:6)+
  labs(
    title = "Association Between True Mean and Average Estimate of Mean",
    x = "True Mean",
    y = "Average Estimate of Mean"
  )
```

```{r}
#Make a second plot showing the average estimate of μ_hat only in samples for which the null was rejected on the y axis and the true value of μ on the x axis
sim_reject = sim_result %>% 
  group_by(mu)%>% 
  filter(p_value<0.05) %>%
  summarize(average_mu = mean(mu_hat))

sim_result%>% 
  group_by(mu)%>%
  summarize(average_mu = mean(mu_hat)) %>% 
  ggplot(aes(x=mu, y= average_mu, color = "Total sample"))+
  geom_point()+
  geom_line()+
  geom_text(aes(label = round(average_mu,3)),vjust = -1)+
  geom_point(data = sim_reject, aes(x= mu, y = average_mu, color = "Rejected samples"))+
  geom_line(data = sim_reject, aes(x= mu, y= average_mu, color = "Rejected samples"))+
  geom_text(data = sim_reject, aes(label = round(average_mu, 3), color = "Rejected samples"), vjust = -1)+
  scale_x_continuous(breaks = 1:6) +
  scale_y_continuous(breaks = 1:6) +
  labs(
    title = "Association between ture mean and average estimates",
    x= "True Mean",
    y= "Average estimate of mean (reject null samples)"
  )
```
* From the plot, we could see the sample average of $\hat{\mu}$ across tests for all samples is approximately equal to the true value of $\mu$.
* for the sample average of $\hat{\mu}$ across data for which the null was rejected, it is difference between the true value of $\mu$ and average of $\hat{\mu}$ when $\mu=0,1,2,3$. and approximately equal to the true value of $\mu$ when $\mu=4,5,6$. 
* As the sample size increase, the probability that the null hypothesis is rejected is increase as well.
* As the true mean gets larger, the effect size becomes larger will leads to a higher probability of correctly rejecting the null hypothesis. When $\mu=4,5,6$, more samples are correctly rejected, the power is increasing as well. So we could observe that the average of $\hat{\mu}$ for rejected samples is approximately equal to the true value of $\mu$.


